{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonyhk9528/Algomap-leetcode/blob/main/Cardiff_NLP_Hackathon_2025_Starter_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cardiff NLP Hackathon 2025 - Starter Code\n",
        "\n",
        "Welcome to Cardiff NLP's second hackathon! Below is some code to get started on the AMPLYFI API and look at some data.\n",
        "\n",
        "====================\n",
        "\n",
        "Note: the API is a real time resource so extra points to projects that can treat it as a continual data stream rather than a one-off data source!\n",
        "\n",
        "Another thing to note about this is that it will affect Amplyfi's servers if you download a silly amount of data. We ask that you only request 100 results per request, but if you have the data you need, try to download it or store it as a variable rather than requesting the exact same data over and over again."
      ],
      "metadata": {
        "id": "J5PoPAgwXukD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAyXKlRkXfT1",
        "outputId": "fd579562-f990-4bf7-dc47-3aeaead8abbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import some libraries\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amplyfi have provided some limits and explanations of what you can query the API for below:\n",
        "\n",
        "`query_text` anything\n",
        "\n",
        "`result_size` <=100\n",
        "\n",
        "`include_highlights` boolean (if True, you get sentences matching keyphrases in the query)\n",
        "\n",
        "`include_smart_tags` boolean (if True, you get back metadata from our \"smart\" market intel classifiers - signals and industries)\n",
        "\n",
        "`ai_answer` can only accept \"basic\", this will take the 5 most relevant docs and answer the query_text based on them\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3nWf0GTmkF1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API endpoint from the newly deployed service\n",
        "\n",
        "API_URL = \"https://zfgp45ih7i.execute-api.eu-west-1.amazonaws.com/sandbox/api/search\"\n",
        "API_KEY = \"GHJ38746G38B7RB46GBER\"\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"x-api-key\": API_KEY\n",
        "}\n",
        "\n",
        "# Edit the below to get different data\n",
        "payload = {\n",
        "  \"query_text\": \"What has Boeing been doing recently?\",\n",
        "  \"result_size\": 10,\n",
        "  \"include_highlights\":True,\n",
        "  \"ai_answer\": \"basic\"\n",
        "}\n",
        "\n",
        "response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
        "json_response = response.json()\n",
        "\n",
        "json_response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB9UJXhho0XV",
        "outputId": "758986c1-5406-493a-e5d1-3b76e8814e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Endpoint request timed out'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_response['results'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "4LYdLzB2OhFA",
        "outputId": "b99e5d01-3837-4cfd-9bef-cab658039a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'results'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-79a4ad1e4cc5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'results'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.json_normalize(json_response['results'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IWGX-TZ8YBTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example Sentiment Analysis"
      ],
      "metadata": {
        "id": "JBLHUuN-qsgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Clean data\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    - Convert to lowercase\n",
        "    - Remove URLs\n",
        "    - Remove punctuation / non-alpha\n",
        "    - Collapse multiple spaces\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove URLs (very basic)\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"\", text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Keep only letters and spaces\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    # Collapse multiple spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_summary'] = df['summary'].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "sN-TP0mPqEmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sentiment analysis example\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_scores(text):\n",
        "    \"\"\"\n",
        "    Returns a dict with these keys:\n",
        "       - neg: negative sentiment score\n",
        "       - neu: neutral score\n",
        "       - pos: positive score\n",
        "       - compound: normalized, weighted composite (-1 to +1)\n",
        "    \"\"\"\n",
        "    return sia.polarity_scores(text)\n",
        "\n",
        "# Apply to each summary\n",
        "df['sentiment'] = df['clean_summary'].apply(get_sentiment_scores)\n",
        "\n",
        "# Split into separate columns if you like\n",
        "df['sent_neg'] = df['sentiment'].apply(lambda d: d['neg'])\n",
        "df['sent_neu'] = df['sentiment'].apply(lambda d: d['neu'])\n",
        "df['sent_pos'] = df['sentiment'].apply(lambda d: d['pos'])\n",
        "df['sent_compound'] = df['sentiment'].apply(lambda d: d['compound'])\n",
        "\n",
        "# Quick look at top 5 compound scores\n",
        "print(df[['clean_summary', 'sent_compound']].sort_values(by='sent_compound', ascending=False).head())\n",
        "print(df[['clean_summary', 'sent_compound']].sort_values(by='sent_compound').head())\n"
      ],
      "metadata": {
        "id": "Le9c5zKPpSQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find index of the most positive (max compound) and most negative (min compound) summaries\n",
        "max_idx = df['sent_compound'].idxmax()\n",
        "min_idx = df['sent_compound'].idxmin()\n",
        "\n",
        "# Retrieve the scores\n",
        "max_score = df.loc[max_idx, 'sent_compound']\n",
        "min_score = df.loc[min_idx, 'sent_compound']\n",
        "\n",
        "# Print the full clean summaries along with their sentiment scores\n",
        "print(\"Most positive summary (compound = {:.3f}):\\n\".format(max_score))\n",
        "print(df.loc[max_idx, 'clean_summary'])\n",
        "\n",
        "\n",
        "print(\"\\n\\nMost negative summary (compound = {:.3f}):\\n\".format(min_score))\n",
        "print(df.loc[min_idx, 'clean_summary'])"
      ],
      "metadata": {
        "id": "_9SPSX-rqYFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Ideas\n",
        "\n",
        "Feel free to use this code to start your own project, and here are some (Chat-GPT generated üò¨) ideas for projects:\n",
        "\n",
        "* Real-Time Sentiment Pulse: Visualize sentiment trends over the past 24-48 hours for any keyword.\n",
        "\n",
        "* One-Click News Brief: Generate a 3-sentence summary of today's top articles on a given topic.\n",
        "\n",
        "* Bias/Slant Detector: Compare headlines from multiple outlets on the same event and label their bias.\n",
        "\n",
        "* Event Timeline Generator: Autofill a chronological list of key dates and summaries for any query.\n",
        "\n",
        "* Breaking News Alert Bot: Push a short alert whenever article volume spikes or sentiment turns extreme.\n",
        "\n",
        "* Multilingual Hashtag Trend Mapper: Show related hashtags and translations across different languages.\n",
        "\n",
        "* Rumor vs. Fact Checker: Verify a user-provided statement against recent reputable sources.\n",
        "\n",
        "* ‚ÄúWhat's Changed?‚Äù Comparator: Highlight how coverage of a topic has shifted from last month to last week.\n",
        "\n",
        "* Geo-Mood Map: Color-code countries by average sentiment or topic intensity on a query.\n",
        "\n",
        "* Voice-Activated News Q&A: Let users speak a question and hear back a 2‚Äì3 sentence summary of current events."
      ],
      "metadata": {
        "id": "P3uEBnGrqxlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dashboard libraries for Python\n",
        "\n",
        "https://shiny.posit.co/py/\n",
        "\n",
        "https://dash.plotly.com/\n",
        "\n",
        "https://streamlit.io/\n",
        "\n"
      ],
      "metadata": {
        "id": "ZfH0xonVrTwz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2lU_lu5pxR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}